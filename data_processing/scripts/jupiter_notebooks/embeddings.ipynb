{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    # LLaMA models\n",
    "    \"llama-3.2-3b\": \"meta-llama/Llama-3.2-3B\",\n",
    "    \"llama-3.1-3b\": \"meta-llama/Llama-3.1-3B\",\n",
    "    \"llama-3.3-70b\": \"meta-llama/Llama-3.3-70B\",\n",
    "    \"llama-2-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \n",
    "    # DeepSeek models\n",
    "    \"deepseek-qwen-32b\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    \"deepseek-llama-70b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "}\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generate embeddings for citation network papers using various language models.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"llama-3.2-3b\", device: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize embedding generator.\n",
    "\n",
    "        Args:\n",
    "            model_name: Model identifier (key from MODEL_CONFIGS or HuggingFace model ID)\n",
    "            device: Device to use ('cuda', 'cpu', or None for auto-detect)\n",
    "        \"\"\"\n",
    "        # Setup logging first\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Resolve model name\n",
    "        self.model_name = MODEL_CONFIGS.get(model_name, model_name)\n",
    "        self.model_type = self._detect_model_type(self.model_name)\n",
    "        \n",
    "        self.device = self._setup_device(device)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        self.logger.info(f\"Initialized with model: {self.model_name}\")\n",
    "        self.logger.info(f\"Model type: {self.model_type}\")\n",
    "    \n",
    "    def _detect_model_type(self, model_name: str) -> str:\n",
    "        \"\"\"Detect model type based on model name.\"\"\"\n",
    "        model_lower = model_name.lower()\n",
    "        \n",
    "        if any(x in model_lower for x in ['sentence-transformers', 'bge', 'e5', 'gte']):\n",
    "            return 'sentence-transformer'\n",
    "        elif any(x in model_lower for x in ['llama', 'qwen', 'mistral', 'deepseek']):\n",
    "            return 'causal-lm'\n",
    "        else:\n",
    "            return 'auto'\n",
    "\n",
    "    def _setup_huggingface_auth(self):\n",
    "        \"\"\"Setup HuggingFace authentication if needed.\"\"\"\n",
    "        hf_token = os.getenv(\"HF_TOKEN\") or os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "        if hf_token:\n",
    "            try:\n",
    "                login(token=hf_token, add_to_git_credential=False)\n",
    "                self.logger.info(\"âœ“ HuggingFace authentication successful\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Authentication failed: {e}\")\n",
    "\n",
    "    def _setup_device(self, device: Optional[str]) -> str:\n",
    "        \"\"\"Setup computation device.\"\"\"\n",
    "        if device:\n",
    "            return device\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.logger.info(\"CUDA available, using GPU\")\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            self.logger.info(\"CUDA not available, using CPU\")\n",
    "            return \"cpu\"\n",
    "\n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Load tokenizer and model based on model type.\"\"\"\n",
    "        self.logger.info(f\"Loading model: {self.model_name}\")\n",
    "        self._setup_huggingface_auth()\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "\n",
    "            # Add padding token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "            # Load model with appropriate dtype\n",
    "            dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
    "            \n",
    "            # Load model based on type\n",
    "            if self.model_type == 'sentence-transformer':\n",
    "                self.model = AutoModel.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=dtype,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "            elif self.model_type == 'causal-lm':\n",
    "                # For causal LMs, we'll use the base model for embeddings\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=dtype,\n",
    "                    trust_remote_code=True\n",
    "                ).base_model\n",
    "            else:\n",
    "                # Auto-detect\n",
    "                try:\n",
    "                    self.model = AutoModel.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=dtype,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "                except:\n",
    "                    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                        self.model_name,\n",
    "                        torch_dtype=dtype,\n",
    "                        trust_remote_code=True\n",
    "                    ).base_model\n",
    "            \n",
    "            self.model.eval()\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "            self.logger.info(\"Model loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings_batch(self, texts: list, batch_size: int = 8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for multiple texts in batches.\n",
    "\n",
    "        Args:\n",
    "            texts: List of input texts\n",
    "            batch_size: Batch size for processing\n",
    "\n",
    "        Returns:\n",
    "            Array of embeddings\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        hidden_size = getattr(self.model.config, 'hidden_size', 768)\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            # Filter out empty texts\n",
    "            valid_texts = [t if t and not pd.isna(t) else \"\" for t in batch_texts]\n",
    "\n",
    "            if all(not t for t in valid_texts):\n",
    "                # All texts are empty, return zero vectors\n",
    "                batch_embeddings = np.zeros((len(batch_texts), hidden_size))\n",
    "            else:\n",
    "                # Tokenize batch\n",
    "                inputs = self.tokenizer(\n",
    "                    valid_texts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to(self.device)\n",
    "\n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "\n",
    "                    # Get embeddings based on model type\n",
    "                    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "                        batch_embeddings = outputs.pooler_output.cpu().numpy()\n",
    "                    else:\n",
    "                        # Mean pooling\n",
    "                        embeddings_tensor = outputs.last_hidden_state\n",
    "                        attention_mask = inputs['attention_mask']\n",
    "\n",
    "                        mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings_tensor.size()).float()\n",
    "                        sum_embeddings = torch.sum(embeddings_tensor * mask_expanded, dim=1)\n",
    "                        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "                        batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "    def process_citation_data(self, \n",
    "                            nodes_csv: str = \"../data/processed/nodes.csv\", \n",
    "                            output_dir: str = \"../embeddings\",\n",
    "                            batch_size: int = 8) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process citation network data and generate embeddings.\n",
    "\n",
    "        Args:\n",
    "            nodes_csv: Path to nodes CSV file\n",
    "            output_dir: Directory to save embeddings\n",
    "            batch_size: Batch size for processing\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with embedding info\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        self.logger.info(f\"Loading data from {nodes_csv}\")\n",
    "        df = pd.read_csv(nodes_csv)\n",
    "\n",
    "        # Extract titles\n",
    "        titles = df['title'].tolist()\n",
    "        paper_ids = df['paper_id'].tolist()\n",
    "\n",
    "        self.logger.info(f\"Processing {len(titles)} papers\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.generate_embeddings_batch(titles, batch_size)\n",
    "\n",
    "        # Prepare output\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save embeddings as numpy array\n",
    "        model_short_name = self.model_name.split('/')[-1].lower().replace('-', '_')\n",
    "        embeddings_file = output_path / f\"{model_short_name}_embeddings.npy\"\n",
    "        np.save(embeddings_file, embeddings)\n",
    "        self.logger.info(f\"Saved embeddings to {embeddings_file}\")\n",
    "\n",
    "        # Save embeddings with paper IDs for reference\n",
    "        embeddings_with_ids = pd.DataFrame({\n",
    "            'paper_id': paper_ids,\n",
    "            **{f'dim_{i}': embeddings[:, i] for i in range(embeddings.shape[1])}\n",
    "        })\n",
    "\n",
    "        csv_file = output_path / f\"{model_short_name}_embeddings.csv\"\n",
    "        embeddings_with_ids.to_csv(csv_file, index=False)\n",
    "        self.logger.info(f\"Saved embeddings CSV to {csv_file}\")\n",
    "\n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model': self.model_name,\n",
    "            'num_papers': len(paper_ids),\n",
    "            'embedding_dim': embeddings.shape[1],\n",
    "            'device': self.device,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'embeddings_file': str(embeddings_file),\n",
    "            'csv_file': str(csv_file),\n",
    "            'metadata': metadata,\n",
    "            'shape': embeddings.shape\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Generate embeddings for citation network\")\n",
    "    parser.add_argument(\n",
    "        \"--model\", \n",
    "        type=str, \n",
    "        default=\"llama-3.2-3b\",\n",
    "        help=\"Model name (key from MODEL_CONFIGS or HuggingFace model ID)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\", \n",
    "        type=int, \n",
    "        default=8,\n",
    "        help=\"Batch size for processing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", \n",
    "        type=str, \n",
    "        default=None,\n",
    "        help=\"Device to use (cuda/cpu)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nodes-csv\", \n",
    "        type=str, \n",
    "        default=\"../data/processed/nodes.csv\",\n",
    "        help=\"Path to nodes CSV file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\", \n",
    "        type=str, \n",
    "        default=\"../embeddings\",\n",
    "        help=\"Output directory for embeddings\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize generator\n",
    "    generator = EmbeddingGenerator(\n",
    "        model_name=args.model,\n",
    "        device=args.device\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    generator.load_model()\n",
    "    \n",
    "    # Process citation data\n",
    "    result = generator.process_citation_data(\n",
    "        nodes_csv=args.nodes_csv,\n",
    "        output_dir=args.output_dir,\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEmbedding generation complete!\")\n",
    "    print(f\"Shape: {result['shape']}\")\n",
    "    print(f\"Saved to: {result['embeddings_file']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "bda4882758048c0"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
